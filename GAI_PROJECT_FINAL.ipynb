{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susub2/Embedded-project/blob/main/%08%08GAI_PROJECT_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGR-bb1MqvFI"
      },
      "outputs": [],
      "source": [
        "!pip install gradio vllm transformers triton PyPDF2 Pillow sentence_transformers numpy typing faiss-gpu spacy pymupdf4llm fitz frontend tools semchunk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import faiss\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import time\n",
        "import semchunk\n",
        "import pymupdf as fitz\n",
        "import pymupdf4llm\n",
        "from vllm import LLM, SamplingParams\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from PIL import Image\n",
        "import hashlib\n",
        "import logging\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
        "llm = LLM(model=\"llava-hf/llava-v1.6-mistral-7b-hf\", dtype='half', max_model_len=8192)\n",
        "sampling_params = SamplingParams(temperature=0.7, max_tokens=512)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "embedder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "chunker = semchunk.chunkerify('gpt-4', 200)"
      ],
      "metadata": {
        "id": "26ISbtvCrrrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PDF íŒŒì¼ RAGë¥¼ ìœ„í•œ Pipeline class\n",
        "\"\"\"\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        # ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸ëœ llmê³¼ sampling_params ì‚¬ìš©\n",
        "        self.llm = llm\n",
        "        self.sampling_params = sampling_params\n",
        "\n",
        "        # embedding\n",
        "        self.embedder = embedder\n",
        "        self.chunker = chunker\n",
        "        self.index = faiss.IndexFlatL2(self.embedder.get_sentence_embedding_dimension())\n",
        "        self.chunks = []\n",
        "        self.processed_files = {} # {file_hash: file_path}\n",
        "\n",
        "    def get_file_hash(self, file_path: str) -> str:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "    def indexing_pdf(self, pdf_path: List[str]):\n",
        "        for pdf in pdf_path:\n",
        "            try:\n",
        "                file_hash = self.get_file_hash(pdf)\n",
        "                if file_hash in self.processed_files:\n",
        "                    logging.info(f\"{pdf} has already been processed before\")\n",
        "                    continue\n",
        "\n",
        "                self.processed_files[file_hash] = pdf\n",
        "                logging.info(f\"Processing new file: {pdf}\")\n",
        "\n",
        "                doc = fitz.open(pdf)\n",
        "                markdown_text = pymupdf4llm.to_markdown(doc)\n",
        "                doc.close()\n",
        "\n",
        "                chunks = self.chunker(markdown_text)\n",
        "                self.chunks.extend(chunks)\n",
        "                chunks_embeddings = self.embedder.encode(chunks)\n",
        "                self.index.add(chunks_embeddings)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in indexing {pdf_path}: {e}\")\n",
        "\n",
        "        logging.info(f\"Processed {len(pdf_path)} files. Total unique files: {len(self.processed_files)}\")\n",
        "\n",
        "    def process_query(self, query: str, top_k: int = 5) -> List[str]:\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "        return [self.chunks[i] for i in indices[0]]\n",
        "\n",
        "    def prompt_template(self, query: str, context: List[str]) -> str:\n",
        "        system_message = \"\"\"You are an AI assistant tasked with answering questions based on provided context. Your role is to:\n",
        "                            1. Carefully analyze the given context\n",
        "                            2. Provide accurate and relevant information\n",
        "                            3. Synthesize a coherent response\n",
        "                            4. Maintain objectivity and clarity\n",
        "                            If the context doesn't contain sufficient information, state so clearly.\"\"\"\n",
        "\n",
        "        context_str = \"\\n\".join([f\"Context {i+1}: {ctx}\" for i, ctx in enumerate(context)])\n",
        "\n",
        "        prompt = f\"\"\"[INST] {system_message}\n",
        "\n",
        "            Relevant information:\n",
        "            {context_str}\n",
        "\n",
        "            User's Quetion: {query}\n",
        "\n",
        "            Instructions:\n",
        "            - Answer the query using only the information provided in the context.\n",
        "            - If the context doesn't contain enough information to fully answer the query, acknowledge this limitation in your response.\n",
        "            - Provide a concise yet comprehensive answer.\n",
        "            - Do not introduce information not present in the given context.\n",
        "            - Provide in complete sentences in English always.\n",
        "            - Check once again your response so that the user can be provided precise information.\n",
        "\n",
        "            Please provide your response below:\n",
        "            [/INST]\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def generate_response(self, query: str, context: List[str]) -> str:\n",
        "        prompt = self.prompt_template(query, context)\n",
        "        output = self.llm.generate([prompt], self.sampling_params)\n",
        "        return output[0].outputs[0].text\n",
        "\n",
        "    def answer_query(self, query: str, top_k: int = 5) -> str:\n",
        "        retrieved_contexts = self.process_query(query, top_k)\n",
        "        return self.generate_response(query, retrieved_contexts)\n",
        "\n",
        "\"\"\"\n",
        "ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ LLaVA Processor class\n",
        "\"\"\"\n",
        "class LLaVAImageQAProcessor:\n",
        "    def __init__(self):\n",
        "        self.llm = llm\n",
        "        self.sampling_params = sampling_params\n",
        "\n",
        "    def get_prompt(self, question: str):\n",
        "        # ê¸°ë³¸ ì„¤ëª… ìš”ì²­ì¸ ê²½ìš°\n",
        "        if question.lower() in [\"ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", \"ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì¤˜\", \"ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì¤˜\"]:\n",
        "            return f\"\"\"[INST] <image>\n",
        "                    Describe this image comprehensively in bullet points.\n",
        "                    Focus on:\n",
        "                    - Main subjects and their characteristics\n",
        "                    - Setting and background\n",
        "                    - Overall mood or atmosphere\n",
        "                    Your response should be in complete sentences.\n",
        "                    [/INST]\"\"\"\n",
        "\n",
        "        # íŠ¹ì • ë¶€ë¶„ì— ëŒ€í•œ ì§ˆë¬¸ì¸ ê²½ìš°\n",
        "        else:\n",
        "            return f\"\"\"[INST] <image>\n",
        "                    Focus specifically on answering this question: {question}\n",
        "                    Provide a detailed response about exactly what was asked.\n",
        "                    Stay focused on the specific aspect mentioned in the question.\n",
        "                    Your response should be in complete sentences and bullet points.\n",
        "                    [/INST]\"\"\"\n",
        "\n",
        "    def process_image(self, image: Image.Image, question: str) -> str:\n",
        "        prompt = self.get_prompt(question)\n",
        "        try:\n",
        "            inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}}\n",
        "            outputs = self.llm.generate(inputs, self.sampling_params)\n",
        "            return outputs[0].outputs[0].text.strip() if outputs else \"Failed to generate response.\"\n",
        "        finally:\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "class GeneralChatProcessor:\n",
        "    def __init__(self):\n",
        "        self.llm = llm\n",
        "        self.sampling_params = sampling_params\n",
        "        self.history = []\n",
        "\n",
        "    def process_query(self, query: str) -> str:\n",
        "        context = \"\\n\".join(self.history[-5:])\n",
        "        prompt = f\"\"\"[INST] You are a friendly and knowledgeable AI assistant. Please respond to the user's question following these guidelines:\n",
        "\n",
        "                      1. Carefully read and understand the question.\n",
        "                      2. Consider the conversation history for context: {context}\n",
        "                      3. Utilize relevant information and knowledge to provide accurate and informative answers.\n",
        "                      4. Write your response clearly and concisely, but include sufficient explanation when necessary.\n",
        "                      5. Provide only fact-based information, and explicitly state when something is uncertain.\n",
        "                      6. Maintain a polite and courteous attitude, considering the user's feelings.\n",
        "                      7. When needed, provide examples or step-by-step instructions.\n",
        "                      8. Only provide information that is ethical and legally appropriate.\n",
        "                      9. Show a welcoming attitude towards additional questions or requests for clarification from the user.\n",
        "\n",
        "                      User's question: {query}\n",
        "\n",
        "                      Please respond following the above guidelines.\n",
        "\n",
        "                      [/INST]\"\"\"\n",
        "\n",
        "        output = self.llm.generate([prompt], self.sampling_params)\n",
        "        response = output[0].outputs[0].text.strip()\n",
        "        self.history.append(f\"User: {query}\")\n",
        "        self.history.append(f\"Assistant: {response}\")\n",
        "\n",
        "        return response\n",
        "\n",
        "\"\"\"\n",
        "ì„¸ì…˜ ê´€ë¦¬ë¥¼ ìœ„í•œ SessionManager class\n",
        "\"\"\"\n",
        "class SessionManager:\n",
        "    def __init__(self):\n",
        "        self.sessions = {\n",
        "            \"Example\": {\n",
        "                \"history\": [],\n",
        "                \"mode\": \"General Chat\",\n",
        "                \"mode_locked\": False,\n",
        "                \"rag_pipeline\": RAGPipeline(),\n",
        "                \"img_processor\": LLaVAImageQAProcessor(),\n",
        "                \"general_processor\": GeneralChatProcessor(),\n",
        "                \"current_image\": None,\n",
        "                \"selected_image_name\": \"\"\n",
        "            }\n",
        "        }\n",
        "        self.current_session = \"Example\"\n",
        "\n",
        "    def create_session(self, session_name: str) -> bool:\n",
        "        if session_name in self.sessions:\n",
        "            return False\n",
        "\n",
        "        self.sessions[session_name] = {\n",
        "            \"history\": [],\n",
        "            \"mode\": \"General Chat\",\n",
        "            \"mode_locked\": False,\n",
        "            \"rag_pipeline\": RAGPipeline(),\n",
        "            \"img_processor\": LLaVAImageQAProcessor(),\n",
        "            \"general_processor\": GeneralChatProcessor(),\n",
        "            \"current_image\": None,\n",
        "            \"selected_image_name\": \"\"\n",
        "        }\n",
        "        self.current_session = session_name\n",
        "        return True\n",
        "\n",
        "    def delete_session(self, session_name: str) -> Tuple[str, dict]:\n",
        "        if len(self.sessions) <= 1:\n",
        "            return None, None\n",
        "\n",
        "        if session_name in self.sessions:\n",
        "            del self.sessions[session_name]\n",
        "            next_session = next(iter(self.sessions.keys()))\n",
        "            self.current_session = next_session\n",
        "            return next_session, self.sessions[next_session]\n",
        "        return None, None\n",
        "\n",
        "    def get_session(self, session_name: str) -> Optional[dict]:\n",
        "        return self.sessions.get(session_name)"
      ],
      "metadata": {
        "id": "4Wmr-Idn48fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GUI êµ¬í˜„ë¶€\n",
        "def create_ui():\n",
        "    session_manager = SessionManager()\n",
        "\n",
        "    custom_css = \"\"\"\n",
        "    .message-box {\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        gap: 0.5rem;\n",
        "    }\n",
        "    .file-btn {\n",
        "        max-width: 40px;\n",
        "    }\n",
        "    .send-btn {\n",
        "        max-width: 40px;\n",
        "    }\n",
        "    .selected-file {\n",
        "        margin: 0.5rem 0;\n",
        "        padding: 0.3rem;\n",
        "        background: #f0f0f0;\n",
        "        border-radius: 4px;\n",
        "        font-size: 0.9em;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=custom_css) as demo:\n",
        "        with gr.Row():\n",
        "            # ì™¼ìª½ íŒ¨ë„: ì„¸ì…˜ ê´€ë¦¬\n",
        "            with gr.Column(scale=1):\n",
        "                new_session_btn = gr.Button(\"+ New Session\")\n",
        "                session_title_input = gr.Textbox(\n",
        "                    label=\"Session Title\",\n",
        "                    visible=False\n",
        "                )\n",
        "                with gr.Column(elem_classes=\"session-container\"):\n",
        "                    gr.Markdown(\"Sessions\")\n",
        "                    session_list = gr.Radio(\n",
        "                        choices=[\"Example\"],\n",
        "                        value=\"Example\",\n",
        "                        label=\"\"\n",
        "                    )\n",
        "                    delete_btn = gr.Button(\"ğŸ—‘ï¸ Delete Session\")\n",
        "\n",
        "            # ì˜¤ë¥¸ìª½ íŒ¨ë„: ì±„íŒ… ì¸í„°í˜ì´ìŠ¤\n",
        "            with gr.Column(scale=3):\n",
        "                current_title = gr.Markdown(\"## Example\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    chat_mode = gr.Radio(\n",
        "                        choices=[\"General Chat\", \"Image Chat\", \"RAG Chat\"],\n",
        "                        value=\"General Chat\",\n",
        "                        label=\"\"\n",
        "                    )\n",
        "\n",
        "                chatbot = gr.Chatbot(\n",
        "                    height=400,\n",
        "                    render_markdown=True,\n",
        "                    show_copy_button=True,\n",
        "                    bubble_full_width=False\n",
        "                )\n",
        "\n",
        "                # ë©”ì‹œì§€ ì…ë ¥ ì˜ì—­\n",
        "                with gr.Row():\n",
        "                    # Image Chat ëª¨ë“œìš© íŒŒì¼ ì—…ë¡œë“œ (ì‘ì€ ë²„íŠ¼)\n",
        "                    with gr.Column(scale=1, visible=False, min_width=50) as image_chat:\n",
        "                        file_upload_image = gr.UploadButton(\n",
        "                            \"ğŸ“\",\n",
        "                            file_types=[\".jpg\", \".jpeg\", \".png\"],\n",
        "                            scale=1\n",
        "                        )\n",
        "\n",
        "                    # ë©”ì‹œì§€ ì…ë ¥ì°½\n",
        "                    with gr.Column(scale=8):\n",
        "                        msg = gr.Textbox(\n",
        "                            show_label=False,\n",
        "                            placeholder=\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",\n",
        "                            container=False\n",
        "                        )\n",
        "\n",
        "                    # ì „ì†¡ ë²„íŠ¼\n",
        "                    with gr.Column(scale=1, min_width=50):\n",
        "                        send_btn = gr.Button(\"â†‘\")\n",
        "\n",
        "                # RAG Chat ëª¨ë“œìš© PDF ì—…ë¡œë“œ (Clear Chatê³¼ ë™ì¼í•œ ë„ˆë¹„)\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=1, visible=False) as rag_chat:\n",
        "                        file_upload_pdf = gr.File(\n",
        "                            label=\"PDF Upload\",\n",
        "                            file_types=[\".pdf\"],\n",
        "                            file_count=\"multiple\"\n",
        "                        )\n",
        "\n",
        "                with gr.Row():\n",
        "                    clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "                with gr.Row(visible=False) as general_file_info:\n",
        "                    selected_image = gr.Textbox(\n",
        "                        label=\"Selected Image\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "                with gr.Row(visible=False) as rag_file_info:\n",
        "                    selected_pdf = gr.Textbox(\n",
        "                        label=\"Selected PDF\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "        # ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "        def process_message(message, file_image, files_pdf, mode, history, session_name):\n",
        "            try:\n",
        "                session = session_manager.get_session(session_name)\n",
        "                if not session:\n",
        "                    return \"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "                current_mode = session[\"mode\"]\n",
        "\n",
        "                if current_mode == \"Image Chat\":\n",
        "                    if file_image:\n",
        "                        with Image.open(file_image) as image:\n",
        "                            if image.mode != 'RGB':\n",
        "                                image = image.convert('RGB')\n",
        "                            session[\"current_image\"] = image.copy()\n",
        "                            session[\"selected_image_name\"] = file_image.name\n",
        "                            image_for_process = image\n",
        "                    elif session[\"current_image\"]:\n",
        "                        image_for_process = session[\"current_image\"]\n",
        "                    else:\n",
        "                        return \"ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "                    question = message if message.strip() else \"ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
        "                    return session[\"img_processor\"].process_image(image_for_process, question)\n",
        "\n",
        "                elif current_mode == \"RAG Chat\":\n",
        "                    if files_pdf:\n",
        "                        pdf_paths = [f.name for f in files_pdf]\n",
        "                        session[\"rag_pipeline\"].indexing_pdf(pdf_paths)\n",
        "                        return f\"{len(pdf_paths)}ê°œì˜ PDFê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "                    return session[\"rag_pipeline\"].answer_query(message)\n",
        "\n",
        "                else:  # General Chat\n",
        "                    return session[\"general_processor\"].process_query(message)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}\")\n",
        "                return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
        "\n",
        "        def chat_mode_change(mode, session_name):\n",
        "            session = session_manager.get_session(session_name)\n",
        "            if not session:\n",
        "                return [gr.update()] * 6\n",
        "            if session[\"mode_locked\"]:\n",
        "                gr.Warning(\"ëŒ€í™”ê°€ ì‹œì‘ëœ í›„ì—ëŠ” ëª¨ë“œë¥¼ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ì„¸ì…˜ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\")\n",
        "                current_mode = session[\"mode\"]\n",
        "            else:\n",
        "                session[\"mode\"] = mode\n",
        "                current_mode = mode\n",
        "\n",
        "            is_image = current_mode == \"Image Chat\"\n",
        "            is_rag = current_mode == \"RAG Chat\"\n",
        "\n",
        "            return [\n",
        "                gr.update(value=current_mode),\n",
        "                gr.update(),  # msgëŠ” í•­ìƒ í‘œì‹œ\n",
        "                gr.update(visible=is_image),\n",
        "                gr.update(visible=is_rag),\n",
        "                gr.update(visible=is_image),\n",
        "                gr.update(visible=is_rag)\n",
        "            ]\n",
        "\n",
        "\n",
        "        # ë©”ì‹œì§€ ì „ì†¡ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "        def send_message(message, file_image, files_pdf, session_name, mode, history):\n",
        "            if not message.strip() and not (file_image or files_pdf):\n",
        "                return history, \"\", None, None, \"\", \"\"\n",
        "\n",
        "            try:\n",
        "                session = session_manager.get_session(session_name)\n",
        "                if not session:\n",
        "                    return history, \"\", None, None, \"\", \"\"\n",
        "\n",
        "                if not session[\"mode_locked\"] and (message.strip() or file_image or files_pdf):\n",
        "                    session[\"mode_locked\"] = True\n",
        "                    session[\"mode\"] = mode\n",
        "\n",
        "                current_mode = session[\"mode\"]\n",
        "                response = process_message(message, file_image if current_mode == \"Image Chat\" else None,\n",
        "                                          files_pdf if current_mode == \"RAG Chat\" else None,\n",
        "                                          current_mode, history, session_name)\n",
        "\n",
        "                if current_mode == \"Image Chat\" and file_image:\n",
        "                    history.append(((file_image.name, file_image), message if message.strip() else None))\n",
        "                elif current_mode == \"RAG Chat\" and files_pdf:\n",
        "                    pdf_names = [f.name for f in files_pdf]\n",
        "                    history.append((f\"Uploaded PDFs: {', '.join(pdf_names)}\", None))\n",
        "                else:\n",
        "                    history.append((None, message))\n",
        "\n",
        "                history.append((None, response))\n",
        "                session[\"history\"] = history\n",
        "\n",
        "                return (history, \"\", None, None,\n",
        "                        session[\"selected_image_name\"] if current_mode == \"Image Chat\" else \"\",\n",
        "                        \", \".join(f.name for f in files_pdf) if files_pdf else \"\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {str(e)}\")\n",
        "                return history, \"\", None, None, \"\", \"\"\n",
        "\n",
        "        # ì„¸ì…˜ ê´€ë¦¬ í•¨ìˆ˜ë“¤\n",
        "        def add_session(title):\n",
        "            if not title:\n",
        "                return gr.update(visible=False), gr.update(choices=list(session_manager.sessions.keys()))\n",
        "\n",
        "            if session_manager.create_session(title):\n",
        "                return gr.update(visible=False), gr.update(choices=list(session_manager.sessions.keys()), value=title)\n",
        "            else:\n",
        "                gr.Warning(\"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì„¸ì…˜ ì´ë¦„ì…ë‹ˆë‹¤.\")\n",
        "                return gr.update(visible=False), gr.update(choices=list(session_manager.sessions.keys()))\n",
        "\n",
        "        def switch_session(session_name):\n",
        "            session = session_manager.get_session(session_name)\n",
        "            if session:\n",
        "                session_manager.current_session = session_name\n",
        "                return (\n",
        "                    f\"## {session_name}\",\n",
        "                    session[\"history\"],\n",
        "                    session[\"mode\"]\n",
        "                )\n",
        "            return current_title, [], chat_mode.value\n",
        "\n",
        "        def delete_session(session_name):\n",
        "            next_session, session_data = session_manager.delete_session(session_name)\n",
        "            if next_session is None:\n",
        "                gr.Warning(\"ë§ˆì§€ë§‰ ì„¸ì…˜ì€ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
        "                return (\n",
        "                    gr.update(choices=list(session_manager.sessions.keys()), value=session_name),\n",
        "                    current_title,\n",
        "                    chatbot,\n",
        "                    chat_mode\n",
        "                )\n",
        "\n",
        "            return (\n",
        "                gr.update(choices=list(session_manager.sessions.keys()), value=next_session),\n",
        "                f\"## {next_session}\",\n",
        "                session_data[\"history\"],\n",
        "                session_data[\"mode\"]\n",
        "            )\n",
        "\n",
        "        def update_selected_file(file):\n",
        "            session = session_manager.get_session(session_manager.current_session)\n",
        "            if session:\n",
        "                session[\"selected_image_name\"] = file.name\n",
        "\n",
        "            is_image = file.name.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "            return (\n",
        "                file.name if is_image else \"\",  # selected_image\n",
        "                \"\" if is_image else file.name,  # selected_pdf\n",
        "                gr.update(visible=is_image),    # general_file_info visibility\n",
        "                gr.update(visible=not is_image) # rag_file_info visibility\n",
        "            )\n",
        "\n",
        "        def clear_chat():\n",
        "            session = session_manager.get_session(session_manager.current_session)\n",
        "            if session:\n",
        "                session[\"current_image\"] = None\n",
        "                session[\"selected_image_name\"] = \"\"\n",
        "            return [], \"\", \"\"\n",
        "\n",
        "        # ì´ë²¤íŠ¸ ë°”ì¸ë”©\n",
        "        new_session_btn.click(\n",
        "            lambda: gr.update(visible=True),\n",
        "            outputs=session_title_input\n",
        "        )\n",
        "\n",
        "        session_title_input.submit(\n",
        "            add_session,\n",
        "            inputs=[session_title_input],\n",
        "            outputs=[session_title_input, session_list]\n",
        "        )\n",
        "\n",
        "        session_list.change(\n",
        "            switch_session,\n",
        "            inputs=[session_list],\n",
        "            outputs=[current_title, chatbot, chat_mode]\n",
        "        )\n",
        "\n",
        "        delete_btn.click(\n",
        "            delete_session,\n",
        "            inputs=[session_list],\n",
        "            outputs=[session_list, current_title, chatbot, chat_mode]\n",
        "        )\n",
        "\n",
        "        send_btn.click(\n",
        "            send_message,\n",
        "            inputs=[msg, file_upload_image, file_upload_pdf, session_list, chat_mode, chatbot],\n",
        "            outputs=[chatbot, msg, file_upload_image, file_upload_pdf, selected_image, selected_pdf]\n",
        "        )\n",
        "\n",
        "        msg.submit(\n",
        "            send_message,\n",
        "            inputs=[msg, file_upload_image, file_upload_pdf, session_list, chat_mode, chatbot],\n",
        "            outputs=[chatbot, msg, file_upload_image, file_upload_pdf, selected_image, selected_pdf]\n",
        "        )\n",
        "\n",
        "        file_upload_image.upload(\n",
        "            update_selected_file,\n",
        "            inputs=[file_upload_image],\n",
        "            outputs=[\n",
        "                selected_image,\n",
        "                selected_pdf,\n",
        "                general_file_info,\n",
        "                rag_file_info\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        chat_mode.change(\n",
        "            chat_mode_change,\n",
        "            inputs=[chat_mode, session_list],\n",
        "            outputs=[chat_mode, msg, image_chat, rag_chat, general_file_info, rag_file_info]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            clear_chat,\n",
        "            outputs=[chatbot, selected_image, selected_pdf]\n",
        "        )\n",
        "\n",
        "        return demo\n",
        "\n",
        "# GUI ì‹¤í–‰\n",
        "demo = create_ui()\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "unJy8Jzv5AWk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}