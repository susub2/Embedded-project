{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susub2/Embedded-project/blob/main/%08%08GAI_PROJECT_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGR-bb1MqvFI"
      },
      "outputs": [],
      "source": [
        "!pip install gradio vllm transformers triton PyPDF2 Pillow sentence_transformers numpy typing faiss-gpu spacy pymupdf4llm fitz frontend tools semchunk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import faiss\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import time\n",
        "import semchunk\n",
        "import pymupdf as fitz\n",
        "import pymupdf4llm\n",
        "from vllm import LLM, SamplingParams\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from PIL import Image\n",
        "import hashlib\n",
        "import logging\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# 전역 변수 초기화\n",
        "llm = LLM(model=\"llava-hf/llava-v1.6-mistral-7b-hf\", dtype='half', max_model_len=8192)\n",
        "sampling_params = SamplingParams(temperature=0.7, max_tokens=512)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "embedder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "chunker = semchunk.chunkerify('gpt-4', 200)"
      ],
      "metadata": {
        "id": "26ISbtvCrrrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PDF 파일 RAG를 위한 Pipeline class\n",
        "\"\"\"\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        # 전역 변수로 선언된 llm과 sampling_params 사용\n",
        "        self.llm = llm\n",
        "        self.sampling_params = sampling_params\n",
        "\n",
        "        # embedding\n",
        "        self.embedder = embedder\n",
        "        self.chunker = chunker\n",
        "        self.index = faiss.IndexFlatL2(self.embedder.get_sentence_embedding_dimension())\n",
        "        self.chunks = []\n",
        "        self.processed_files = {} # {file_hash: file_path}\n",
        "\n",
        "    def get_file_hash(self, file_path: str) -> str:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "    def indexing_pdf(self, pdf_path: List[str]):\n",
        "        for pdf in pdf_path:\n",
        "            try:\n",
        "                file_hash = self.get_file_hash(pdf)\n",
        "                if file_hash in self.processed_files:\n",
        "                    logging.info(f\"{pdf} has already been processed before\")\n",
        "                    continue\n",
        "\n",
        "                self.processed_files[file_hash] = pdf\n",
        "                logging.info(f\"Processing new file: {pdf}\")\n",
        "\n",
        "                doc = fitz.open(pdf)\n",
        "                markdown_text = pymupdf4llm.to_markdown(doc)\n",
        "                doc.close()\n",
        "\n",
        "                chunks = self.chunker(markdown_text)\n",
        "                self.chunks.extend(chunks)\n",
        "                chunks_embeddings = self.embedder.encode(chunks)\n",
        "                self.index.add(chunks_embeddings)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in indexing {pdf_path}: {e}\")\n",
        "\n",
        "        logging.info(f\"Processed {len(pdf_path)} files. Total unique files: {len(self.processed_files)}\")\n",
        "\n",
        "    def process_query(self, query: str, top_k: int = 5) -> List[str]:\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "        return [self.chunks[i] for i in indices[0]]\n",
        "\n",
        "    def prompt_template(self, query: str, context: List[str]) -> str:\n",
        "        system_message = \"\"\"You are an AI assistant tasked with answering questions based on provided context. Your role is to:\n",
        "                            1. Carefully analyze the given context\n",
        "                            2. Provide accurate and relevant information\n",
        "                            3. Synthesize a coherent response\n",
        "                            4. Maintain objectivity and clarity\n",
        "                            If the context doesn't contain sufficient information, state so clearly.\"\"\"\n",
        "\n",
        "        context_str = \"\\n\".join([f\"Context {i+1}: {ctx}\" for i, ctx in enumerate(context)])\n",
        "\n",
        "        prompt = f\"\"\"[INST] {system_message}\n",
        "\n",
        "            Relevant information:\n",
        "            {context_str}\n",
        "\n",
        "            User's Quetion: {query}\n",
        "\n",
        "            Instructions:\n",
        "            - Answer the query using only the information provided in the context.\n",
        "            - If the context doesn't contain enough information to fully answer the query, acknowledge this limitation in your response.\n",
        "            - Provide a concise yet comprehensive answer.\n",
        "            - Do not introduce information not present in the given context.\n",
        "            - Provide in complete sentences in English always.\n",
        "            - Check once again your response so that the user can be provided precise information.\n",
        "\n",
        "            Please provide your response below:\n",
        "            [/INST]\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def generate_response(self, query: str, context: List[str]) -> str:\n",
        "        prompt = self.prompt_template(query, context)\n",
        "        output = self.llm.generate([prompt], self.sampling_params)\n",
        "        return output[0].outputs[0].text\n",
        "\n",
        "    def answer_query(self, query: str, top_k: int = 5) -> str:\n",
        "        retrieved_contexts = self.process_query(query, top_k)\n",
        "        return self.generate_response(query, retrieved_contexts)\n",
        "\n",
        "\"\"\"\n",
        "이미지 처리를 위한 LLaVA Processor class\n",
        "\"\"\"\n",
        "class LLaVAImageQAProcessor:\n",
        "    def __init__(self):\n",
        "        self.llm = llm\n",
        "        self.sampling_params = sampling_params\n",
        "\n",
        "    def get_prompt(self, question: str):\n",
        "        # 기본 설명 요청인 경우\n",
        "        if question.lower() in [\"이 이미지에 대해 설명해줘\", \"이미지를 설명해줘\", \"이 이미지를 설명해줘\"]:\n",
        "            return f\"\"\"[INST] <image>\n",
        "                    Describe this image comprehensively in bullet points.\n",
        "                    Focus on:\n",
        "                    - Main subjects and their characteristics\n",
        "                    - Setting and background\n",
        "                    - Overall mood or atmosphere\n",
        "                    Your response should be in complete sentences.\n",
        "                    [/INST]\"\"\"\n",
        "\n",
        "        # 특정 부분에 대한 질문인 경우\n",
        "        else:\n",
        "            return f\"\"\"[INST] <image>\n",
        "                    Focus specifically on answering this question: {question}\n",
        "                    Provide a detailed response about exactly what was asked.\n",
        "                    Stay focused on the specific aspect mentioned in the question.\n",
        "                    Your response should be in complete sentences and bullet points.\n",
        "                    [/INST]\"\"\"\n",
        "\n",
        "    def process_image(self, image: Image.Image, question: str) -> str:\n",
        "        prompt = self.get_prompt(question)\n",
        "        try:\n",
        "            inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}}\n",
        "            outputs = self.llm.generate(inputs, self.sampling_params)\n",
        "            return outputs[0].outputs[0].text.strip() if outputs else \"Failed to generate response.\"\n",
        "        finally:\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "class GeneralChatProcessor:\n",
        "    def __init__(self):\n",
        "        self.llm = llm\n",
        "        self.sampling_params = sampling_params\n",
        "        self.history = []\n",
        "\n",
        "    def process_query(self, query: str) -> str:\n",
        "        context = \"\\n\".join(self.history[-5:])\n",
        "        prompt = f\"\"\"[INST] You are a friendly and knowledgeable AI assistant. Please respond to the user's question following these guidelines:\n",
        "\n",
        "                      1. Carefully read and understand the question.\n",
        "                      2. Consider the conversation history for context: {context}\n",
        "                      3. Utilize relevant information and knowledge to provide accurate and informative answers.\n",
        "                      4. Write your response clearly and concisely, but include sufficient explanation when necessary.\n",
        "                      5. Provide only fact-based information, and explicitly state when something is uncertain.\n",
        "                      6. Maintain a polite and courteous attitude, considering the user's feelings.\n",
        "                      7. When needed, provide examples or step-by-step instructions.\n",
        "                      8. Only provide information that is ethical and legally appropriate.\n",
        "                      9. Show a welcoming attitude towards additional questions or requests for clarification from the user.\n",
        "\n",
        "                      User's question: {query}\n",
        "\n",
        "                      Please respond following the above guidelines.\n",
        "\n",
        "                      [/INST]\"\"\"\n",
        "\n",
        "        output = self.llm.generate([prompt], self.sampling_params)\n",
        "        response = output[0].outputs[0].text.strip()\n",
        "        self.history.append(f\"User: {query}\")\n",
        "        self.history.append(f\"Assistant: {response}\")\n",
        "\n",
        "        return response\n",
        "\n",
        "\"\"\"\n",
        "세션 관리를 위한 SessionManager class\n",
        "\"\"\"\n",
        "class SessionManager:\n",
        "    def __init__(self):\n",
        "        self.sessions = {\n",
        "            \"Example\": {\n",
        "                \"history\": [],\n",
        "                \"mode\": \"General Chat\",\n",
        "                \"mode_locked\": False,\n",
        "                \"rag_pipeline\": RAGPipeline(),\n",
        "                \"img_processor\": LLaVAImageQAProcessor(),\n",
        "                \"general_processor\": GeneralChatProcessor(),\n",
        "                \"current_image\": None,\n",
        "                \"selected_image_name\": \"\"\n",
        "            }\n",
        "        }\n",
        "        self.current_session = \"Example\"\n",
        "\n",
        "    def create_session(self, session_name: str) -> bool:\n",
        "        if session_name in self.sessions:\n",
        "            return False\n",
        "\n",
        "        self.sessions[session_name] = {\n",
        "            \"history\": [],\n",
        "            \"mode\": \"General Chat\",\n",
        "            \"mode_locked\": False,\n",
        "            \"rag_pipeline\": RAGPipeline(),\n",
        "            \"img_processor\": LLaVAImageQAProcessor(),\n",
        "            \"general_processor\": GeneralChatProcessor(),\n",
        "            \"current_image\": None,\n",
        "            \"selected_image_name\": \"\"\n",
        "        }\n",
        "        self.current_session = session_name\n",
        "        return True\n",
        "\n",
        "    def delete_session(self, session_name: str) -> Tuple[str, dict]:\n",
        "        if len(self.sessions) <= 1:\n",
        "            return None, None\n",
        "\n",
        "        if session_name in self.sessions:\n",
        "            del self.sessions[session_name]\n",
        "            next_session = next(iter(self.sessions.keys()))\n",
        "            self.current_session = next_session\n",
        "            return next_session, self.sessions[next_session]\n",
        "        return None, None\n",
        "\n",
        "    def get_session(self, session_name: str) -> Optional[dict]:\n",
        "        return self.sessions.get(session_name)"
      ],
      "metadata": {
        "id": "4Wmr-Idn48fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GUI 구현부\n",
        "def create_ui():\n",
        "    session_manager = SessionManager()\n",
        "\n",
        "    custom_css = \"\"\"\n",
        "    .message-box {\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        gap: 0.5rem;\n",
        "    }\n",
        "    .file-btn {\n",
        "        max-width: 40px;\n",
        "    }\n",
        "    .send-btn {\n",
        "        max-width: 40px;\n",
        "    }\n",
        "    .selected-file {\n",
        "        margin: 0.5rem 0;\n",
        "        padding: 0.3rem;\n",
        "        background: #f0f0f0;\n",
        "        border-radius: 4px;\n",
        "        font-size: 0.9em;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=custom_css) as demo:\n",
        "        with gr.Row():\n",
        "            # 왼쪽 패널: 세션 관리\n",
        "            with gr.Column(scale=1):\n",
        "                new_session_btn = gr.Button(\"+ New Session\")\n",
        "                session_title_input = gr.Textbox(\n",
        "                    label=\"Session Title\",\n",
        "                    visible=False\n",
        "                )\n",
        "                with gr.Column(elem_classes=\"session-container\"):\n",
        "                    gr.Markdown(\"Sessions\")\n",
        "                    session_list = gr.Radio(\n",
        "                        choices=[\"Example\"],\n",
        "                        value=\"Example\",\n",
        "                        label=\"\"\n",
        "                    )\n",
        "                    delete_btn = gr.Button(\"🗑️ Delete Session\")\n",
        "\n",
        "            # 오른쪽 패널: 채팅 인터페이스\n",
        "            with gr.Column(scale=3):\n",
        "                current_title = gr.Markdown(\"## Example\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    chat_mode = gr.Radio(\n",
        "                        choices=[\"General Chat\", \"Image Chat\", \"RAG Chat\"],\n",
        "                        value=\"General Chat\",\n",
        "                        label=\"\"\n",
        "                    )\n",
        "\n",
        "                chatbot = gr.Chatbot(\n",
        "                    height=400,\n",
        "                    render_markdown=True,\n",
        "                    show_copy_button=True,\n",
        "                    bubble_full_width=False\n",
        "                )\n",
        "\n",
        "                # 메시지 입력 영역\n",
        "                with gr.Row():\n",
        "                    # Image Chat 모드용 파일 업로드 (작은 버튼)\n",
        "                    with gr.Column(scale=1, visible=False, min_width=50) as image_chat:\n",
        "                        file_upload_image = gr.UploadButton(\n",
        "                            \"📎\",\n",
        "                            file_types=[\".jpg\", \".jpeg\", \".png\"],\n",
        "                            scale=1\n",
        "                        )\n",
        "\n",
        "                    # 메시지 입력창\n",
        "                    with gr.Column(scale=8):\n",
        "                        msg = gr.Textbox(\n",
        "                            show_label=False,\n",
        "                            placeholder=\"메시지를 입력하세요...\",\n",
        "                            container=False\n",
        "                        )\n",
        "\n",
        "                    # 전송 버튼\n",
        "                    with gr.Column(scale=1, min_width=50):\n",
        "                        send_btn = gr.Button(\"↑\")\n",
        "\n",
        "                # RAG Chat 모드용 PDF 업로드 (Clear Chat과 동일한 너비)\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=1, visible=False) as rag_chat:\n",
        "                        file_upload_pdf = gr.File(\n",
        "                            label=\"PDF Upload\",\n",
        "                            file_types=[\".pdf\"],\n",
        "                            file_count=\"multiple\"\n",
        "                        )\n",
        "\n",
        "                with gr.Row():\n",
        "                    clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "                with gr.Row(visible=False) as general_file_info:\n",
        "                    selected_image = gr.Textbox(\n",
        "                        label=\"Selected Image\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "                with gr.Row(visible=False) as rag_file_info:\n",
        "                    selected_pdf = gr.Textbox(\n",
        "                        label=\"Selected PDF\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "        # 메시지 처리 함수\n",
        "        def process_message(message, file_image, files_pdf, mode, history, session_name):\n",
        "            try:\n",
        "                session = session_manager.get_session(session_name)\n",
        "                if not session:\n",
        "                    return \"세션을 찾을 수 없습니다.\"\n",
        "\n",
        "                current_mode = session[\"mode\"]\n",
        "\n",
        "                if current_mode == \"Image Chat\":\n",
        "                    if file_image:\n",
        "                        with Image.open(file_image) as image:\n",
        "                            if image.mode != 'RGB':\n",
        "                                image = image.convert('RGB')\n",
        "                            session[\"current_image\"] = image.copy()\n",
        "                            session[\"selected_image_name\"] = file_image.name\n",
        "                            image_for_process = image\n",
        "                    elif session[\"current_image\"]:\n",
        "                        image_for_process = session[\"current_image\"]\n",
        "                    else:\n",
        "                        return \"이미지를 먼저 업로드해주세요.\"\n",
        "\n",
        "                    question = message if message.strip() else \"이 이미지에 대해 설명해주세요.\"\n",
        "                    return session[\"img_processor\"].process_image(image_for_process, question)\n",
        "\n",
        "                elif current_mode == \"RAG Chat\":\n",
        "                    if files_pdf:\n",
        "                        pdf_paths = [f.name for f in files_pdf]\n",
        "                        session[\"rag_pipeline\"].indexing_pdf(pdf_paths)\n",
        "                        return f\"{len(pdf_paths)}개의 PDF가 성공적으로 처리되었습니다. 이제 문서에 대해 질문할 수 있습니다.\"\n",
        "\n",
        "                    return session[\"rag_pipeline\"].answer_query(message)\n",
        "\n",
        "                else:  # General Chat\n",
        "                    return session[\"general_processor\"].process_query(message)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"메시지 처리 중 오류: {str(e)}\")\n",
        "                return f\"오류가 발생했습니다: {str(e)}\"\n",
        "\n",
        "        def chat_mode_change(mode, session_name):\n",
        "            session = session_manager.get_session(session_name)\n",
        "            if not session:\n",
        "                return [gr.update()] * 6\n",
        "            if session[\"mode_locked\"]:\n",
        "                gr.Warning(\"대화가 시작된 후에는 모드를 변경할 수 없습니다. 새 세션을 만들어주세요.\")\n",
        "                current_mode = session[\"mode\"]\n",
        "            else:\n",
        "                session[\"mode\"] = mode\n",
        "                current_mode = mode\n",
        "\n",
        "            is_image = current_mode == \"Image Chat\"\n",
        "            is_rag = current_mode == \"RAG Chat\"\n",
        "\n",
        "            return [\n",
        "                gr.update(value=current_mode),\n",
        "                gr.update(),  # msg는 항상 표시\n",
        "                gr.update(visible=is_image),\n",
        "                gr.update(visible=is_rag),\n",
        "                gr.update(visible=is_image),\n",
        "                gr.update(visible=is_rag)\n",
        "            ]\n",
        "\n",
        "\n",
        "        # 메시지 전송 처리 함수\n",
        "        def send_message(message, file_image, files_pdf, session_name, mode, history):\n",
        "            if not message.strip() and not (file_image or files_pdf):\n",
        "                return history, \"\", None, None, \"\", \"\"\n",
        "\n",
        "            try:\n",
        "                session = session_manager.get_session(session_name)\n",
        "                if not session:\n",
        "                    return history, \"\", None, None, \"\", \"\"\n",
        "\n",
        "                if not session[\"mode_locked\"] and (message.strip() or file_image or files_pdf):\n",
        "                    session[\"mode_locked\"] = True\n",
        "                    session[\"mode\"] = mode\n",
        "\n",
        "                current_mode = session[\"mode\"]\n",
        "                response = process_message(message, file_image if current_mode == \"Image Chat\" else None,\n",
        "                                          files_pdf if current_mode == \"RAG Chat\" else None,\n",
        "                                          current_mode, history, session_name)\n",
        "\n",
        "                if current_mode == \"Image Chat\" and file_image:\n",
        "                    history.append(((file_image.name, file_image), message if message.strip() else None))\n",
        "                elif current_mode == \"RAG Chat\" and files_pdf:\n",
        "                    pdf_names = [f.name for f in files_pdf]\n",
        "                    history.append((f\"Uploaded PDFs: {', '.join(pdf_names)}\", None))\n",
        "                else:\n",
        "                    history.append((None, message))\n",
        "\n",
        "                history.append((None, response))\n",
        "                session[\"history\"] = history\n",
        "\n",
        "                return (history, \"\", None, None,\n",
        "                        session[\"selected_image_name\"] if current_mode == \"Image Chat\" else \"\",\n",
        "                        \", \".join(f.name for f in files_pdf) if files_pdf else \"\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"메시지 전송 중 오류: {str(e)}\")\n",
        "                return history, \"\", None, None, \"\", \"\"\n",
        "\n",
        "        # 세션 관리 함수들\n",
        "        def add_session(title):\n",
        "            if not title:\n",
        "                return gr.update(visible=False), gr.update(choices=list(session_manager.sessions.keys()))\n",
        "\n",
        "            if session_manager.create_session(title):\n",
        "                return gr.update(visible=False), gr.update(choices=list(session_manager.sessions.keys()), value=title)\n",
        "            else:\n",
        "                gr.Warning(\"이미 존재하는 세션 이름입니다.\")\n",
        "                return gr.update(visible=False), gr.update(choices=list(session_manager.sessions.keys()))\n",
        "\n",
        "        def switch_session(session_name):\n",
        "            session = session_manager.get_session(session_name)\n",
        "            if session:\n",
        "                session_manager.current_session = session_name\n",
        "                return (\n",
        "                    f\"## {session_name}\",\n",
        "                    session[\"history\"],\n",
        "                    session[\"mode\"]\n",
        "                )\n",
        "            return current_title, [], chat_mode.value\n",
        "\n",
        "        def delete_session(session_name):\n",
        "            next_session, session_data = session_manager.delete_session(session_name)\n",
        "            if next_session is None:\n",
        "                gr.Warning(\"마지막 세션은 삭제할 수 없습니다\")\n",
        "                return (\n",
        "                    gr.update(choices=list(session_manager.sessions.keys()), value=session_name),\n",
        "                    current_title,\n",
        "                    chatbot,\n",
        "                    chat_mode\n",
        "                )\n",
        "\n",
        "            return (\n",
        "                gr.update(choices=list(session_manager.sessions.keys()), value=next_session),\n",
        "                f\"## {next_session}\",\n",
        "                session_data[\"history\"],\n",
        "                session_data[\"mode\"]\n",
        "            )\n",
        "\n",
        "        def update_selected_file(file):\n",
        "            session = session_manager.get_session(session_manager.current_session)\n",
        "            if session:\n",
        "                session[\"selected_image_name\"] = file.name\n",
        "\n",
        "            is_image = file.name.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "            return (\n",
        "                file.name if is_image else \"\",  # selected_image\n",
        "                \"\" if is_image else file.name,  # selected_pdf\n",
        "                gr.update(visible=is_image),    # general_file_info visibility\n",
        "                gr.update(visible=not is_image) # rag_file_info visibility\n",
        "            )\n",
        "\n",
        "        def clear_chat():\n",
        "            session = session_manager.get_session(session_manager.current_session)\n",
        "            if session:\n",
        "                session[\"current_image\"] = None\n",
        "                session[\"selected_image_name\"] = \"\"\n",
        "            return [], \"\", \"\"\n",
        "\n",
        "        # 이벤트 바인딩\n",
        "        new_session_btn.click(\n",
        "            lambda: gr.update(visible=True),\n",
        "            outputs=session_title_input\n",
        "        )\n",
        "\n",
        "        session_title_input.submit(\n",
        "            add_session,\n",
        "            inputs=[session_title_input],\n",
        "            outputs=[session_title_input, session_list]\n",
        "        )\n",
        "\n",
        "        session_list.change(\n",
        "            switch_session,\n",
        "            inputs=[session_list],\n",
        "            outputs=[current_title, chatbot, chat_mode]\n",
        "        )\n",
        "\n",
        "        delete_btn.click(\n",
        "            delete_session,\n",
        "            inputs=[session_list],\n",
        "            outputs=[session_list, current_title, chatbot, chat_mode]\n",
        "        )\n",
        "\n",
        "        send_btn.click(\n",
        "            send_message,\n",
        "            inputs=[msg, file_upload_image, file_upload_pdf, session_list, chat_mode, chatbot],\n",
        "            outputs=[chatbot, msg, file_upload_image, file_upload_pdf, selected_image, selected_pdf]\n",
        "        )\n",
        "\n",
        "        msg.submit(\n",
        "            send_message,\n",
        "            inputs=[msg, file_upload_image, file_upload_pdf, session_list, chat_mode, chatbot],\n",
        "            outputs=[chatbot, msg, file_upload_image, file_upload_pdf, selected_image, selected_pdf]\n",
        "        )\n",
        "\n",
        "        file_upload_image.upload(\n",
        "            update_selected_file,\n",
        "            inputs=[file_upload_image],\n",
        "            outputs=[\n",
        "                selected_image,\n",
        "                selected_pdf,\n",
        "                general_file_info,\n",
        "                rag_file_info\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        chat_mode.change(\n",
        "            chat_mode_change,\n",
        "            inputs=[chat_mode, session_list],\n",
        "            outputs=[chat_mode, msg, image_chat, rag_chat, general_file_info, rag_file_info]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            clear_chat,\n",
        "            outputs=[chatbot, selected_image, selected_pdf]\n",
        "        )\n",
        "\n",
        "        return demo\n",
        "\n",
        "# GUI 실행\n",
        "demo = create_ui()\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "unJy8Jzv5AWk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}